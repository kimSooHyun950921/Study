# 04-1 로지스틱 회귀
- n개의 클래스로 분류하는 다중 분류방법은 어떻게 분류 할것인가?
-   ``` z = aW + bL + cD + dH + eW +f```
    - a, b, c, d, e, f를 구해야함
    - 확률이 되려면 0~1 이되어야함
    - sigmoid 함수: 0~1 사이의 범위를 벗어나지 않게 만들기위해서 만든 함수
- 이진 분류 방법: sigmoid $$ {1}\div{(1+e^{-z})}$$
- 다중 분류 방법: softmax 
    - $$ e\_sum = e^{z1} + e^{z2} + ...$$
    - $$ s1 = e^{z1}/e\_sum, s2 = e^{z2}/e\_sum$$
- 로지스틱 회귀의 강도 제어방식도 릿지방식과 라쏘방식으로 가능
# 04-2 확률적 경사하강법
- 훈련한 모델을 버리지 않고 새로운 데이터에 대해서만 조금씩 훈련하는 방법
    - 점진적학습 / 온라인 학습
    - 점진적학습중 하나: 확률적 경사하강법
- 경사하강법 종류: 샘플을 몇개씩 꺼내서 학습하는가?
    - 확률적 경사하강법: 한개씩 꺼내기
    - 미니배치 경사하강법: 여러개씩 꺼내기
    - 배치 경사하강법: 전체 꺼냄
- 손실 함수
    - 샘플하나에대한 손실
    - cf) 비용함수: 훈련세트에 있는 모든 샘플에대한 손실 함수
- 로지스틱 손실 함수: 이진 크로스엔트로피 손실 함수
- 다중분류의 손실 함수: 크로스엔트로피 손실함수
- 회귀의 손실함수: 평균 제곱 오차
- scikit learn에서는 SGDClassifier 사용
- SGD --> Stochastic Gradient Descent